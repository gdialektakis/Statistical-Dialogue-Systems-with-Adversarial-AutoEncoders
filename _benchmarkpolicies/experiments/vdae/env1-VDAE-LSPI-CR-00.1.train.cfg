[GENERAL]
domains = CamRestaurants
singledomain = True
tracedialog = 0
seed = 07051991

[exec_config]
configdir = _benchmarkpolicies/experiments/vdae
logfiledir = _benchmarklogs/experiments/vdae
numtrainbatches = 10
traindialogsperbatch = 300
numbatchtestdialogs = 300
numtestdialogs = 300
trainerrorrate = 0
testerrorrate = 0
testeverybatch = True
trainsourceiteration = 0
save_step = 300
autoencoder = true
transfer = false
transfer_autoencoder_type = dense_multi
single_autoencoder_type = variational_dense_denoising

[logging]
usecolor = False
screen_level = results
file_level = debug
file = _benchmarklogs/experiments/vdae/env1-VDAE-LSPI-CR-seed7051991-00.00.1-10.train.log

[agent]
maxturns = 25

[usermodel]
usenewgoalscenarios = True
oldstylepatience = False
patience = 4,6
configfile = config/sampledUM.cfg

[errormodel]
nbestsize = 1
confusionmodel = RandomConfusions
nbestgeneratormodel = SampledNBestGenerator
confscorer = additive

[summaryacts]
maxinformslots = 5
informmask = True
requestmask = True
informcountaccepted = 4
byemask = True

[policy]
policydir = _LSPIweights
belieftype = belieftracking.baseline.FocusTracker
useconfreq = False
learning = True
policytype = lspi
policytype2 = policy.LSPIPolicy.LSPIPolicy
startwithhello = False
inpolicyfile = _LSPIweights/env1-VDAE-LSPI-CR-00.0
outpolicyfile = _LSPIweights/env1-VDAE-LSPI-CR-00.1
usestack = False
processstack = 10
save_step = 300

[lspipolicy]
phitype = block
pcafile = MGLpolicies/pcafile.pca

[lspi_CamRestaurants]
random = False
discount = 0.95

[lspi_Laptops11]
random = False
discount = 0.95

[lspi_SFRestaurants]
random = False
discount = 0.95

[eval]
rewardvenuerecommended = 0
penaliseallturns = True
wrongvenuepenalty = 0
notmentionedvaluepenalty = 0
successmeasure = objective
successreward = 20

