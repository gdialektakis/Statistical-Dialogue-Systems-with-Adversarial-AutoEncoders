# Domains: SFRestaurants,CamRestaurants,Laptops11
# Error model: different error rate per domain, DSTC2 confscorer and nbestgenerator
# Masks: on

###### General parameters ######

[GENERAL]
domains = SFRestaurants,CamRestaurants,Laptops11
singledomain = False
tracedialog = 0
seed = 07051991

[exec_config]
domain = Multi
configdir = _dqnconfigs
logfiledir = _dqnlogs
numtrainbatches = 20
traindialogsperbatch = 100
numbatchtestdialogs =  150
trainsourceiteration = 0
numtestdialogs =  0
trainerrorrate = 15
testerrorrate  = 15
testeverybatch = True
autoencoder = True
autoencoder_type = src_cnn

[logging]
usecolor = False
screen_level = results
file_level = dial
file = auto

###### Environment parameters ######

[simulate]
mindomainsperdialog = 1
maxdomainsperdialog = 1
domainsampling = roundrobin

[agent]
maxturns = 25

[usermodel]
usenewgoalscenarios = True
oldstylepatience = False
patience = 5
configfile =config/sampledUM.cfg

[errormodel]
nbestsize = 5
confusionmodel = LevenshteinConfusions
nbestgeneratormodel = DSTC2NBestGenerator
confscorer = DSTC2
configfile = var

[errormodel_CamRestaurants]
errorrate = 10
configfile = config/set1-ErrorModel.cfg

[errormodel_SFRestaurants]
errorrate = 15
configfile = config/set3-ErrorModel.cfg

[errormodel_Laptops11]
errorrate = 20
configfile = config/set2-ErrorModel.cfg

[summaryacts]
maxinformslots = 5
informmask = True
requestmask = True
informcountaccepted = 4
byemask = True

###### Dialogue Manager parameters ######

[policy]
policydir = _dqnpolicies
belieftype = focus
useconfreq = False
learning = True
policytype = dqn
startwithhello = False
inpolicyfile = auto 
outpolicyfile = auto

[dqnpolicy]
maxiter = 6000
gamma = 0.98
learning_rate = 0.001
tau = 0.02
replay_type = vanilla
#minibatch_size = 128
minibatch_size = 16
capacity = 3000
exploration_type = e-greedy
episodeNum= 0.0
epsilon_start = 0.5
epsilon_end = 0.0
state_type = full
features = ["discourseAct", "method", "requested", "full", "lastActionInformNone", "offerHappened", "inform_info"]
max_k = 5
learning_algorithm = dqn
architecture = vanilla
h1_size = 250
h2_size = 75
training_frequency = 2
n_samples = 1
stddev_var_mu = 0.01
stddev_var_logsigma = 0.01
mean_log_sigma = 0.000001
sigma_prior = 1.5
alpha =0.85
alpha_divergence =False
sigma_eps = 0.01
delta = 1.0
beta = 0.95
is_threshold = 5.0
train_iters_per_episode = 1

[dqnpolicy_CamRestaurants]
num_in = 268

[dqnpolicy_SFRestaurants]
num_in = 636

[dqnpolicy_Laptops11]
num_in = 257

[dqnpolicy_CamHotels]
num_in = 111

[dqnpolicy_SFHotels]
num_in = 438

[dqnpolicy_Laptops6]
num_in = 268

[dqnpolicy_TV]
num_in = 188

###### Evaluation parameters ######

[eval]
rewardvenuerecommended=0
penaliseallturns = True
wrongvenuepenalty = 0
notmentionedvaluepenalty = 0
successmeasure = objective
successreward = 20
